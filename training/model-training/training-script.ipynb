{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://www.kaggle.com/code/phandai/food101-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:47.701388Z",
     "iopub.status.busy": "2025-01-09T02:52:47.701199Z",
     "iopub.status.idle": "2025-01-09T02:52:56.092488Z",
     "shell.execute_reply": "2025-01-09T02:52:56.091528Z",
     "shell.execute_reply.started": "2025-01-09T02:52:47.701364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score,classification_report\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T18:11:24.934218Z",
     "iopub.status.busy": "2025-01-08T18:11:24.933701Z",
     "iopub.status.idle": "2025-01-08T18:11:24.959426Z",
     "shell.execute_reply": "2025-01-08T18:11:24.958256Z",
     "shell.execute_reply.started": "2025-01-08T18:11:24.934180Z"
    }
   },
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.093855Z",
     "iopub.status.busy": "2025-01-09T02:52:56.093502Z",
     "iopub.status.idle": "2025-01-09T02:52:56.097923Z",
     "shell.execute_reply": "2025-01-09T02:52:56.097002Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.093813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGE_BASE_DIR = \"/kaggle/input/food41/images\"\n",
    "META_BASE_DIR = \"/kaggle/input/food41/meta/meta\"\n",
    "SAVE_WEIGHT_DIR = \"/kaggle/working/weight\" \n",
    "SAVE_OUTPUT_DIR = \"/kaggle/working/output\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.099129Z",
     "iopub.status.busy": "2025-01-09T02:52:56.098868Z",
     "iopub.status.idle": "2025-01-09T02:52:56.118229Z",
     "shell.execute_reply": "2025-01-09T02:52:56.117518Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.099109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_workers, batch_size = 4, 64\n",
    "resized_width = 224\n",
    "resized_height = 224\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.119369Z",
     "iopub.status.busy": "2025-01-09T02:52:56.119118Z",
     "iopub.status.idle": "2025-01-09T02:52:56.135827Z",
     "shell.execute_reply": "2025-01-09T02:52:56.134918Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.119349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = 'resnet50'\n",
    "optim_lr = 5e-4\n",
    "optim_weight_decay = 1e-5\n",
    "\n",
    "scheduler_name = 'ReduceLROnPlateau' \n",
    "# scheduler_name = 'CosineAnnealing'\n",
    "\n",
    "scheduler_params = {\n",
    "    'patience': 2, # for ReduceLROnPlateau\n",
    "    'factor': 0.5,\n",
    "    'min_lr': 1e-6,\n",
    "}\n",
    "\n",
    "# scheduler_params = {\n",
    "#     'T_max': num_epochs,  # for CosineAnnealing\n",
    "#     'min_lr': 1e-6,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.137064Z",
     "iopub.status.busy": "2025-01-09T02:52:56.136754Z",
     "iopub.status.idle": "2025-01-09T02:52:56.198151Z",
     "shell.execute_reply": "2025-01-09T02:52:56.197410Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.137020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load json for train and test\n",
    "with open(f\"{META_BASE_DIR}/train.json\", 'r') as file:\n",
    "    train_json = json.load(file)\n",
    "with open(f\"{META_BASE_DIR}/test.json\", 'r') as file:\n",
    "    test_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.199196Z",
     "iopub.status.busy": "2025-01-09T02:52:56.198948Z",
     "iopub.status.idle": "2025-01-09T02:52:56.203934Z",
     "shell.execute_reply": "2025-01-09T02:52:56.203034Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.199176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now, let convert those json format to dataframe\n",
    "def json_to_dataframe(data_json):\n",
    "    data_rows = []\n",
    "    for label, image_paths in data_json.items():\n",
    "        for image_path in image_paths:\n",
    "            data_rows.append([label, image_path])\n",
    "    df = pd.DataFrame(data_rows, columns=['label', 'image_path'])\n",
    "    df['filepath'] = IMAGE_BASE_DIR+'/' + df['image_path'] +'.jpg'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.206857Z",
     "iopub.status.busy": "2025-01-09T02:52:56.206558Z",
     "iopub.status.idle": "2025-01-09T02:52:56.477388Z",
     "shell.execute_reply": "2025-01-09T02:52:56.476741Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.206835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = json_to_dataframe(train_json)\n",
    "df_test = json_to_dataframe(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.479624Z",
     "iopub.status.busy": "2025-01-09T02:52:56.479288Z",
     "iopub.status.idle": "2025-01-09T02:52:56.485870Z",
     "shell.execute_reply": "2025-01-09T02:52:56.484877Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.479580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.487075Z",
     "iopub.status.busy": "2025-01-09T02:52:56.486788Z",
     "iopub.status.idle": "2025-01-09T02:52:56.508419Z",
     "shell.execute_reply": "2025-01-09T02:52:56.507174Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.487031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_classes = df_train['label'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.509823Z",
     "iopub.status.busy": "2025-01-09T02:52:56.509478Z",
     "iopub.status.idle": "2025-01-09T02:52:56.524821Z",
     "shell.execute_reply": "2025-01-09T02:52:56.523769Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.509783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df_train['label'])\n",
    "df_train['num_label'] = label_encoder.transform(df_train['label'])\n",
    "df_test['num_label'] = label_encoder.transform(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.525963Z",
     "iopub.status.busy": "2025-01-09T02:52:56.525710Z",
     "iopub.status.idle": "2025-01-09T02:52:56.539642Z",
     "shell.execute_reply": "2025-01-09T02:52:56.538761Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.525942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with 'filepath' and 'label' columns.\n",
    "            transforms (albumentations.Compose): Augmentation pipeline.\n",
    "            label_encoder (LabelEncoder): Fitted label encoder. If None, a new one will be created.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Convert string labels to numeric IDs\n",
    "        self.numeric_labels = df['num_label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.df.iloc[idx]\n",
    "        img_path = record['filepath']\n",
    "        \n",
    "        # Verify if the image path exists\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"Image file {img_path} does not exist.\")\n",
    "\n",
    "        # Load and convert image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Apply transforms if any\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        # Get numeric label\n",
    "        label = self.numeric_labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential Augmentation Techniques\n",
    "1. Horizontal and Vertical Flips\n",
    "2. Random Rotations\n",
    "3. Random Cropping and Resizing\n",
    "4. Color Jitter (Brightness, Contrast, Saturation, Hue)\n",
    "5. Scaling and Zooming\n",
    "6. Random Shear and Affine Transformations\n",
    "7. Random Grayscale Conversion\n",
    "8. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.540977Z",
     "iopub.status.busy": "2025-01-09T02:52:56.540586Z",
     "iopub.status.idle": "2025-01-09T02:52:56.554205Z",
     "shell.execute_reply": "2025-01-09T02:52:56.553065Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.540945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose([\n",
    "            A.Transpose(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10),\n",
    "            ], p=0.7),\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(blur_limit=3),\n",
    "                A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "            ], p=0.5),\n",
    "            A.OneOf([\n",
    "                A.OpticalDistortion(distort_limit=0.5),  # Reduced distort_limit\n",
    "                A.GridDistortion(num_steps=5, distort_limit=0.5), # Reduced distort_limit\n",
    "            ], p=0.3), # Reduced probability\n",
    "            A.Resize(resized_height, resized_width),\n",
    "            A.Cutout(max_h_size=int(resized_height * 0.15), max_w_size=int(resized_width * 0.15), num_holes=1, p=0.7),  # Potentially smaller cutout\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "        ], p=1.0) # Overall probability of applying the transform\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.CenterCrop(height=resized_height, width=resized_width),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:56.555412Z",
     "iopub.status.busy": "2025-01-09T02:52:56.555178Z",
     "iopub.status.idle": "2025-01-09T02:52:57.808275Z",
     "shell.execute_reply": "2025-01-09T02:52:57.806884Z",
     "shell.execute_reply.started": "2025-01-09T02:52:56.555384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = FoodDataset(df_train, transforms=get_transforms(mode=\"train\"))\n",
    "\n",
    "# Function to display images\n",
    "def visualize_augmentations(dataset, num_samples=10, cols=5):\n",
    "    \"\"\"\n",
    "    Visualizes augmented images from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to visualize.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        cols (int): Number of columns in the grid.\n",
    "    \"\"\"\n",
    "    dataset_copy = copy.deepcopy(dataset)\n",
    "    transforms = dataset_copy.transforms.transforms\n",
    "    transforms_filtered = [t for t in transforms if not isinstance(t, (A.Normalize, ToTensorV2))]\n",
    "    dataset_copy.transforms = A.Compose(transforms_filtered)\n",
    "    \n",
    "    rows = num_samples // cols + int(num_samples % cols > 0)\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(cols * 3, rows * 3))\n",
    "    ax = ax.flatten() if num_samples > 1 else [ax]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(0, len(dataset_copy))\n",
    "        image, _ = dataset_copy[idx]  # Ignore the label\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).cpu().numpy()\n",
    "            image = np.clip(image, 0, 1)\n",
    "        else:\n",
    "            image = image.astype(np.uint8)\n",
    "        ax[i].imshow(image)\n",
    "        ax[i].axis('off')\n",
    "    for j in range(num_samples, len(ax)):\n",
    "        ax[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some augmented images\n",
    "visualize_augmentations(train_dataset, num_samples=10, cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold and train-val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.809687Z",
     "iopub.status.busy": "2025-01-09T02:52:57.809407Z",
     "iopub.status.idle": "2025-01-09T02:52:57.815129Z",
     "shell.execute_reply": "2025-01-09T02:52:57.814168Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.809665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "# Select fold 0\n",
    "fold = 0\n",
    "def get_train_val_from_fold(fold,val=False):\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X=df_train, y=df_train['label'])):\n",
    "        if fold_idx == fold:\n",
    "            print(f\"Selected Fold: {fold}\")\n",
    "            train_df = df_train.iloc[train_idx].reset_index(drop=True)\n",
    "            val_df = df_train.iloc[val_idx].reset_index(drop=True)\n",
    "            if val: return train_df, val_df, val_idx\n",
    "            return train_df, val_df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.816493Z",
     "iopub.status.busy": "2025-01-09T02:52:57.816231Z",
     "iopub.status.idle": "2025-01-09T02:52:57.831790Z",
     "shell.execute_reply": "2025-01-09T02:52:57.830773Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.816471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loader_from_fold(fold, num_workers, batch_size,val=False):\n",
    "    if val: train_df, val_df, val_idx = get_train_val_from_fold(fold,val=True)\n",
    "    else: train_df, val_df = get_train_val_from_fold(fold)\n",
    "    train_dataset = FoodDataset(\n",
    "        df=train_df, \n",
    "        transforms=get_transforms(mode=\"train\")\n",
    "    )\n",
    "    val_dataset = FoodDataset(\n",
    "        df=val_df, \n",
    "        transforms=get_transforms(mode=\"val\")\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    if val: return train_loader,val_loader, val_idx \n",
    "    return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model | Criterion | Optimizer | Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.832987Z",
     "iopub.status.busy": "2025-01-09T02:52:57.832775Z",
     "iopub.status.idle": "2025-01-09T02:52:57.892127Z",
     "shell.execute_reply": "2025-01-09T02:52:57.891211Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.832970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Set Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.893249Z",
     "iopub.status.busy": "2025-01-09T02:52:57.892928Z",
     "iopub.status.idle": "2025-01-09T02:52:57.905298Z",
     "shell.execute_reply": "2025-01-09T02:52:57.904552Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.893222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_name,device=device):\n",
    "    model = timm.create_model(model_name, num_classes=num_classes, pretrained=True)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.906352Z",
     "iopub.status.busy": "2025-01-09T02:52:57.906068Z",
     "iopub.status.idle": "2025-01-09T02:52:57.919090Z",
     "shell.execute_reply": "2025-01-09T02:52:57.918363Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.906318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_scheduler(scheduler_name, optimizer, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a learning rate scheduler based on name.\n",
    "    \n",
    "    Args:\n",
    "        scheduler_name (str): Name of the scheduler ('ReduceLROnPlateau' or 'CosineAnnealing')\n",
    "        optimizer: PyTorch optimizer\n",
    "        **kwargs: Additional arguments for specific schedulers\n",
    "    \n",
    "    Returns:\n",
    "        scheduler: PyTorch scheduler\n",
    "        is_metric_based (bool): Whether scheduler needs validation metrics\n",
    "    \"\"\"\n",
    "    scheduler_name = scheduler_name.lower()\n",
    "    \n",
    "    if scheduler_name == 'reducelronplateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=kwargs.get('patience', 2),\n",
    "            factor=kwargs.get('factor', 0.5),\n",
    "            min_lr=kwargs.get('min_lr', 1e-6),\n",
    "        )\n",
    "        return scheduler, True\n",
    "    \n",
    "    elif scheduler_name == 'cosineanealing':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=kwargs.get('T_max', 10),\n",
    "            eta_min=kwargs.get('min_lr', 0),\n",
    "        )\n",
    "        return scheduler, False\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {scheduler_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.920216Z",
     "iopub.status.busy": "2025-01-09T02:52:57.919876Z",
     "iopub.status.idle": "2025-01-09T02:52:57.937209Z",
     "shell.execute_reply": "2025-01-09T02:52:57.936375Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.920195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_crit_opt_sche(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=optim_lr, weight_decay=optim_weight_decay)\n",
    "    scheduler, is_metric_based = create_scheduler(scheduler_name, optimizer, **scheduler_params)\n",
    "    return criterion, optimizer, scheduler, is_metric_based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.938314Z",
     "iopub.status.busy": "2025-01-09T02:52:57.938025Z",
     "iopub.status.idle": "2025-01-09T02:52:57.947446Z",
     "shell.execute_reply": "2025-01-09T02:52:57.946679Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.938284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, verbose=0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    if verbose == 1:\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    else:\n",
    "        progress_bar = enumerate(train_loader)\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        data, target = batch\n",
    "        \n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "        if verbose == 1:\n",
    "            # Update progress bar\n",
    "            progress_bar.set_description(f\"Train Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.948446Z",
     "iopub.status.busy": "2025-01-09T02:52:57.948166Z",
     "iopub.status.idle": "2025-01-09T02:52:57.965010Z",
     "shell.execute_reply": "2025-01-09T02:52:57.964287Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.948417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Validation Loop ---\n",
    "def validate(model, val_loader, criterion, device,verbose=0):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if verbose == 1:\n",
    "            progress_bar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "        else: progress_bar = enumerate(val_loader)\n",
    "        for batch_idx, (data, target) in progress_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            if verbose == 1:\n",
    "                progress_bar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.966134Z",
     "iopub.status.busy": "2025-01-09T02:52:57.965879Z",
     "iopub.status.idle": "2025-01-09T02:52:57.986007Z",
     "shell.execute_reply": "2025-01-09T02:52:57.985273Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.966106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model_name, fold_num, train_loader, val_loader, num_epochs, device, SAVE_WEIGHT_DIR, verbose=0):\n",
    "    \"\"\"\n",
    "    Training function that tracks metrics history for visualization.\n",
    "    \n",
    "    Returns:\n",
    "        history (dict): Dictionary containing lists of metrics for each epoch\n",
    "    \"\"\"\n",
    "    model = load_model(model_name)\n",
    "    criterion, optimizer, scheduler, is_metric_based = setup_crit_opt_sche(model)\n",
    "    # Initialize history dictionary to store metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device,verbose)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store metrics in history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}| Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}| Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        if verbose == 1:\n",
    "            print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Update learning rate using the scheduler\n",
    "        if scheduler:\n",
    "            if is_metric_based:\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            os.makedirs(SAVE_WEIGHT_DIR, exist_ok=True)\n",
    "            best_epoch = epoch + 1\n",
    "            # Save both model weights and training history\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            torch.save(\n",
    "                checkpoint,\n",
    "                os.path.join(SAVE_WEIGHT_DIR, f\"{model_name}_fold_{fold_num}.pth\")\n",
    "            )\n",
    "            if verbose ==1:\n",
    "                print(f\"Model saved at epoch {epoch+1}\")\n",
    "        if verbose ==1: print(\"-\" * 50)\n",
    "    print('Best epoch:',best_epoch)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:57.987095Z",
     "iopub.status.busy": "2025-01-09T02:52:57.986819Z",
     "iopub.status.idle": "2025-01-09T02:52:58.002641Z",
     "shell.execute_reply": "2025-01-09T02:52:58.001928Z",
     "shell.execute_reply.started": "2025-01-09T02:52:57.987068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # --- Test training ---\n",
    "# num_epochs = 1\n",
    "# SAVE_WEIGHT_DIR = \"/kaggle/working\" \n",
    "# train(model_name, fold, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, SAVE_WEIGHT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T02:52:58.006262Z",
     "iopub.status.busy": "2025-01-09T02:52:58.006066Z",
     "iopub.status.idle": "2025-01-09T03:05:11.644405Z",
     "shell.execute_reply": "2025-01-09T03:05:11.643177Z",
     "shell.execute_reply.started": "2025-01-09T02:52:58.006245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Training ---\n",
    "print(\"MODEL NAME:\", model_name)\n",
    "print()\n",
    "for fold_num in range(n_splits):\n",
    "    train_loader,val_loader = get_loader_from_fold(fold_num, num_workers, batch_size)\n",
    "    train(model_name, fold_num, train_loader, val_loader, num_epochs, device, SAVE_WEIGHT_DIR, verbose)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1864,
     "sourceId": 33884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
